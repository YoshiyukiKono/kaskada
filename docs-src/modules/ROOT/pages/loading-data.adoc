= Loading Data 

Kaskada stores data in _tables_. Tables consist of multiple rows, and
each row is a value of the same type.
The xref:developing:tables.adoc[Tables] section of the reference docs has more information about managing tables.

== File Format

Any data loaded into a table must include all columns used in the table definition.
The full schema of a table is inferred from the data loaded into it.
At the moment, all data loaded into a table must have the same schema.

The Kaskada compute engine expects uploaded file data to be either CSV or parquet version 2. 

Additionally, it expects the following:

* The `time_column` (and all other date-time columns in your dataset)
should be a type that can be xref:fenl:data-model.adoc#type-coercion[cast] to a xref:fenl:data-model.adoc#scalars[timestamp], for example an integer or RFC3330-formatted string.
* If `subsort_id` is user-defined, the combination of the `group_column` (entity id), `time_column` and `subsort_column` should guarantee that each row is unique.  

== Loading data with the CLI

Files can be loaded into a table using the CLI.
When loading files with the CLI, the given path must be local to the Kaskada service.

The file must be encoded as either CSV or Parquet version 2.

[source,bash]
----
cli load \
  --table Purchase \
  --file-type csv \
  --file-path file://path/to/purchases.csv
----


== Loading data with Python

Data can be loaded into a table in multiple ways, depending on where it is located.

=== From a Local File

Local files can be uploaded directly to Kaskada without converting them
to a dataframe. However, the files must be in a specific format. See
xref:reference:expected-file-format[Expected File Format] for details.

[source,python]
----
from kaskada import table

fullPathToFile = "/content/drive/place/thing/purchases.parquet"
table.upload_file("Purchases", fullPathToFile)
----

This uploads the contents of the file to the Purchases table.

=== From a Pandas dataframe

[source,python]
----
from kaskada import table
import pandas

# A sample Parquet file provided by Kaskada for testing
purchases_url = "https://drive.google.com/uc?export=download&id=1SLdIw9uc0RGHY-eKzS30UBhN0NJtslkk"

# Read the file into a Pandas Dataframe
purchases = pandas.read_parquet(purchases_url)

# Upload the dataframe's contents to the Purchase table
table.load_dataframe("Purchase", purchases)
----

The result of running `load_dataframe` returns a `data_token_id`. The
data token ID is a unique reference to the data currently stored in the
system.

[source,json]
----
data_token_id: "aa2***a6b9"
----

The file is added to the table.

//// 
=== From Amazon S3

Data can be loaded directly from Amazon S3 into a Kaskada table. Loading
from S3 requires the following:

* AWS Access Key - The access key with READ permissions to the bucket
and path (optional).
* AWS Secret Access Key - The secret key associated with the access key
(optional).
* Path - The path to the object. One of the following:
** https://s3.Region.amazonaws.com/bucket-name/key-name[Virtual Hosted
Path] - E.g.
https://s3.Region.amazonaws.com/bucket-name/key-name.parquet
** Region, Bucket, Key

[NOTE]
.Security and Credentials
====
 Kaskada does not store the provided
credentials in any manner. The API only has access to the credentials
throughout the load data call. If no access credentials are provided,
the object must have public read permissions.
====

[source,python]
----
from kaskada import table

TABLE_NAME = 'Purchase'

EXTERNAL_AWS_ACCESS_KEY = '<AWS_ACCESS_KEY'
EXTERNAL_AWS_SECRET_KEY = '<AWS_SECRET_KEY>'
S3_PATH = 'events/2022/purchases.parquet'
BUCKET = 'production.company'
REGION = 'us-west-2'

table.upload_from_s3(
    TABLE_NAME,
    access_key=EXTERNAL_AWS_ACCESS_KEY, 
    secret=EXTERNAL_AWS_SECRET_KEY, 
    bucket=BUCKET,
    key=S3_PATH, 
    region=REGION
)
----

The contents of the parquet object in S3 are transferred to Kaskada and
added to the Purchase table.
////