= Loading Data 

Kaskada stores data in _tables_. Tables consist of multiple rows, and
each row is a value of the same type.
The xref:developing:tables.adoc[Tables] section of the reference docs has more information about managing tables.

== File Format

Any data loaded into a table must include all columns used in the table definition.
The full schema of a table is inferred from the data loaded into it.
At the moment, all data loaded into a table must have the same schema.

The Kaskada compute engine expects uploaded file data to be either CSV or parquet version 2. 

Additionally, it expects the following:

* The `time_column` (and all other date-time columns in your dataset)
should be a type that can be xref:fenl:data-model.adoc#type-coercion[cast] to a xref:fenl:data-model.adoc#scalars[timestamp], for example an integer or RFC3330-formatted string.
* If `subsort_id` is user-defined, the combination of the `group_column` (entity id), `time_column` and `subsort_column` should guarantee that each row is unique.  

== File Location

Kaskada supports loading files from different kinds of file storage.
The file storage system is specified with the file path protocol prefix, for example `file:` or `s3:`.

=== Local Storage

Data can be loaded from the local disk using the `file:` protocol.
When loading from disk, the file path must identify a file accessible to the Kaskada service.
Local storage is not reccommended when using a remote Kaskada service.

=== AWS S3 Storage

Data can be loaded from AWS S3 (or compatible stores such as Minio) using the `s3:` protocol.
When loading non-public objects from S3, the Kaskada service must be configured with credentials.
Credentials are configured using environment variables.

The following environment variables are used to configure credentials:

* `AWS_ACCESS_KEY_ID`: AWS credential key.
* `AWS_SECRET_ACCESS_KEY`: AWS credential secret.
* `AWS_DEFAULT_REGION`: The AWS S3 region to use if not specified.
* `AWS_ENDPOINT`: The S3 endpoint to connect to.
* `AWS_SESSION_TOKEN`: The session token. Session tokens are required for credentials created by assuming an IAM role.
* `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI`: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html
* `AWS_ALLOW_HTTP`: Set to “true” to permit HTTP connections without TLS

== Loading data with Python

Data can be loaded into a table using the `table.load()` function

[source,python]
----
from kaskada import table

fullPathToFile = "file://content/drive/place/thing/purchases.parquet"
table.load("Purchases", fullPathToFile)
----

[TIP]
.Required format
The files must be in a specific format. 
See xref:reference:expected-file-format[Expected File Format] for details.

This loads the contents of the file to the Purchases table.

The result of running `load()` returns a `data_token_id``. The data token ID is a unique reference to the data currently stored in the system.

[source,bash]
----
data_token_id: "aa2***a6b9"
----

== Loading data with the CLI

Files can be loaded into a table using the CLI.
When loading files with the CLI, the given path must be local to the Kaskada service.

The file must be encoded as either CSV or Parquet version 2.

[source,bash]
----
cli load \
  --table Purchase \
  --file-type csv \
  --file-path file://path/to/purchases.csv
----

The result of running `cli load` returns a `data_token_id``. The data token ID is a unique reference to the data currently stored in the system.

[source,bash]
----
data_token_id: "aa2***a6b9"
----