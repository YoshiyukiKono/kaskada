= Hello World (Jupyter)

== Installation

To use Kaskada within a Jupyter notebook you'll need to have the following pieces of software installed 

. https://realpython.com/installing-python/[Python] (version 3.6 and above) 
. https://docs.jupyter.org/en/latest/install/notebook-classic.html[Jupyter] 

Once you have both prerequisites installed ensure that you can run them. 
Open a terminal in your OS (command line prompt on Windows) and check the output of the following commands 

.Verifying Kaskada prerequisites. The output shown here is from an Ubuntu system--the output on your machine may vary.
[,shell]
----
$ python --version
Python 3.10.6


$ jupyter --version
Selected Jupyter core packages...
IPython          : 7.34.0
ipykernel        : 6.17.0
ipywidgets       : 8.0.2
jupyter_client   : 7.4.4
jupyter_core     : 4.11.2
jupyter_server   : 1.21.0
jupyterlab       : 3.6.1
nbclient         : 0.7.0
nbconvert        : 7.2.3
nbformat         : 5.7.0
notebook         : 6.5.2
qtconsole        : 5.3.2
traitlets        : 5.5.0
----


=== Kaskada Client Installation

The first step in using Kaskada in a notebook is to install the Kaskada Python client package. 
Open a terminal in your OS and using `pip` install the Kaskada Python client.

.Installing Kaskada using pip
[,shell]
----
pip install kaskada 
----

[NOTE]
.Pip and pip3 
====
Depending on you Python installation and configuration you may have `pip3` instead of `pip` available in your terminal. 
If you do have `pip3` replace `pip` with `pip3` in your command, i.e., `pip3 install kaskada`
====

Now that we have everything installed let's fire up a notebook and get the remaining components of Kaskada installed.

Using a terminal start a new Jupyter notebook using the command 

.Start a Jupyter notebook.
[,shell]
----
jupyter notebook
----

The jupyter command should activate your browser and you can open a new notebook. 
Create a new code cell in your new notebook and enter the following code in the code cell. 

[,python]
----
from kaskada.api.session import LocalBuilder

session = LocalBuilder().build()
----

Run this cell inside your notebook and you should see some output similar to the following 

.Sample output of Kaskada's installation within a Jupyter notebook.
image::kaskada-install-output.png[Sample output of Kaskada installation within a Jupyter notebook]

This command imports the client's `LocalBuilder` and uses this builder to create a session. 
This is the first time we are running the builder on this machine. 
The builder will download (if needed) the latest release of Kaskada's components from GitHub and then run these components on your local machine. 
The command will generate some output during the download, install and run process. 

Let's now create a small table and write a simple query to see that everything is working correctly with our setup. 

=== Enable the Kaskada magic command 

Kaskada's client includes notebook customizations that allow us to write queries in the Fenl language but also receive and render the results of our queries in our notebooks. 
We need to enable these customizations first before we can use them. 

So in a new code cell input the following command and run this cell. 

.Enable fenlmagic in this notebook 
[,python]
----
%load_ext fenlmagic
----

Now we can start a code cell in our notebook with the first line being the string `%%fenl` to indicate that this cell will contain code in the Fenl language. 
The special string `%%fenl` will also connect to the Kaskada components that we installed that will execute the query and report back any results to our notebook. 

Congratulations, you now have Kaskada locally installed and you can start loading and querying your data using Kaskada inside a Jupyter notebook. 

== Loading Data 

Kaskada stores data in _tables_. Tables consist of multiple rows, and
each row is a value of the same type.

=== Creating a Table

When creating a table, you must provide some information about how each
row should be interpreted. You must describe:

* A field containing the time associated with each row
(`time_column_name`). The time should refer to when the event occurred.
* An initial xref:fenl:entities[entity] key associated with each row
(`entity_key_column_name`). The entity should identify a _thing_ in the
world that each event is associated with. Don't worry too much about
picking the "right" value here - it's easy to change the entity key in
Fenl.
* A subsort column associated with each row (`subsort_column_name`).
This value is used to order rows associated with the same time value.

For more information about these fields, see:
xref:how-to-guides:expected-file-format[Expected File Format]

[source,python]
----
from kaskada import table

table.create_table(
  table_name = "Purchase",
  time_column_name = "purchase_time",
  entity_key_column_name = "customer_id",
  subsort_column_name = "subsort_id",
)
----

This creates a table named `Purchase`. Any data loaded into this table
must have a timestamp field named `purchase_time`, a field named
`customer_id`, and a field named `subsort_id`.

[TIP]
.Idiomatic Kaskada
====
We like to use CamelCase to name tables because it
helps distinguish data sources from transformed values and function
names.
====

The response from the `create_table` is a `table` object with contents
similar to:

[source,json]
----
table {
  table_id: "76b***2e5"
  table_name: "Purchase"
  time_column_name: "purchase_time"
  entity_key_column_name: "customer_id"
  subsort_column_name: "subsort_id"
  create_time {
    seconds: 1634250064
    nanos: 422017488
  }
  update_time {
    seconds: 1634250064
    nanos: 422017488
  }
}
request_details {
  request_id: "fe6bed41fa29cea6ca85fe20bea6ef4a"
}
----

=== Loading Data

Now that we've created a table, we're ready to load some data into it.

Data can be loaded into a table in multiple ways. In this example we'll
load the contents of a Pandas dataframe into the table. To learn about
the different ways data can be loaded into a table, see the
xref:developing:tables.adoc#uploading-data["Uploading Data"
section of the "Working with Data"] page.

[source,python]
----
import pandas

# A sample Parquet file provided by Kaskada for testing
purchases_url = "https://drive.google.com/uc?export=download&id=1SLdIw9uc0RGHY-eKzS30UBhN0NJtslkk"

# Read the file into a Pandas Dataframe
purchases = pandas.read_parquet(purchases_url)

# Upload the dataframe's contents to the Purchase table
table.load_dataframe("Purchase", purchases)
----

The result of running `load_dataframe` is a `data_token_id`. The
data token ID is a unique reference to the data currently stored in the
system.

[source,json]
----
data_token_id: "aa2***a6b9"
request_details {
  request_id: "fe6bed41fa29cea6ca85fe20bea6ef4b"
}
----

The file is transferred to Kaskada and it's content added to the table.

=== Inspecting the Table's Contents

To verify the file was loaded as expected you can use the table list
endpoint to see all the tables defined for your user and the files
loaded into each:

[source,python]
----
table.list_tables()
----

`list_tables` shows all the tables accessible by the user and returns a
`list` of `table`. The table created above is shown here:

[source,json]
----
tables {
  table_id: "76b***2e5"
  table_name: "Purchase"
  time_column_name: "purchase_time"
  entity_key_column_name: "customer_id"
  subsort_column_name: "subsort_id"
  create_time {
    seconds: 1634067588
    nanos: 312567086
  }
  update_time {
    seconds: 1634067603
    nanos: 70745776
  }
  version: 1
}
request_details {
  request_id: "fe6bed41fa29cea6ca85fe20bea6ef4c"
}
----

After executing this block, all tables that have been defined are
returned.

For more help with tables and loading data, see xref:developing:tables.adoc[Reference -
Working with Tables]

== Querying Data

=== Writing Queries

You can write queries in a number of ways with Kaskada. Here we start
with fenlmagic because these queries are not persistent. As you are
iterating in Jupyter it can be helpful to build up your feature and time
selection as you go, once you'd like to persist a query, check out our
article on xref:developing:views.adoc[Sharing Queries].

You can make Fenl queries by prefixing a query block with `%%fenl`. The
query results will be computed and returned as a Pandas dataframe. The
query content starts on the next line and includes the rest of the code
block's contents.

Let's start by looking at the Purchase table without any filters, this
query will return all of the columns and rows contained in a table:

[source,Fenl]
----
%%fenl
Purchase
----

[NOTE]
----
This table is intentionally small so that you can get to know
queries with Kaskada.
----

As you begin to better understand your data you can start using
aggregations over your data such as the `max()` function:

[source,Fenl]
----
%%fenl
{
   max_purchase: Purchase.amount | max(),
}
----

These results may be surprising if you were expecting a single value,
this is a feature, not a bug!

Computations in Fenl are temporal: they produce a time-series of values
describing the full history of a computation's results. Temporal
computation allows Fenl to capture what an expression's value would have
been at arbitrary times in the past.

Fenl values can time-travel forward through time. Time travel allows
combining the result of different computations at different points in
time. Because values can only travel forward in time, Fenl prevents
information about the future from "leaking" into the past.

Read more in the xref:fenl:language-guide.adoc[Fenl
Language Guide]

Now we can start building up our features. To reduce the set of columns
output in your query, you can define a record with the curly braces
`{ }` and name the columns with a label shown on the left of the `:` in
the below query. In order to debug your features, we recommend including
the time and the entity with each query so that you can walk through the
results in time:

[source,Fenl]
----
%%fenl
{
    time: Purchase.purchase_time,
    entity: Purchase.customer_id,
    max_amount: Purchase.amount | max(),
    min_amount: Purchase.amount | min(),
}
----

[TIP]
----
The result of a previous cell in Jupyter is available to be saved
by setting a variable to the result temporarily stored as `_`. You can
then interact with these results at a typical dataframe such as
displaying the columns or plotting a histogram:
----

[source,IPython]
----
df_explore = _
df_explore.dataframe.columns
----

For more help writing queries, see xref:developing:queries.adoc[Reference -
Writing Queries]
