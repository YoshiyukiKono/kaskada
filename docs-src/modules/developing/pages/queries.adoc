= Queries

Kaskada computes results from the data you've loaded by executing _queries_.
A query describes a computation to perform and configures how the query should be executed.

Kaskada queries are witten in a language called Fenl, which is designed to make it easy to describe computations over events.
To learn more about Fenl:

* The xref:fenl:fenl-quick-start.adoc[Quick Start] is an overview of the query language.
* The xref:fenl:language-guide.adoc[Language Guide] explains the design and theory behind the languge.
* The xref:fenl:catalog.adoc[Function Catalog] documents the operations and functions provided by Fenl.
* The xref:fenl:fenl-faq.adoc[FAQ] answers some comment questions about using Fenl.

== Query syntax quickstart

Kaskada's query language builds on the lessons of 50+ years of query language design to provide a declarative, composable, easy-to-read, and type-safe way of describing computations related to time. 
The following is a quick overview of the query language's main features and syntax.

=== Stateful aggregations

Aggregate events to produce a continuous timeline whose value can be observed at arbitrary points in time.

[source,Fenl]
----
{
  max_review_to_date: max(review.stars),
}
----

[TIP]
.What are the curly brackets for?
====
Every query needs to return a xref:fenl:working-with-records.adoc[Record].
Records allow one or more values to be grouped into a single row.
You can create a record using the syntax `{key: value, key2: value2}`.
====

=== Automatic joins

Every expression is associated with an “entity”, allowing tables and expressions to be automatically joined. Entities eliminate redundant boilerplate code.

[source,Fenl]
----
{
  purchases_per_page_view: count(purchase) / count(pageview),
}
----

=== Event-based windowing

Collect events as you move through time, and aggregate them with respect to other events. Ordered aggregation makes it easy to describe temporal interactions.

[source,Fenl]
----
{
  pageviews_since_last_purchase:  count(pageview, window=since(purchase)),
  spend_since_last_review:        count(purchase, window=since(review)),
}
----

=== Pipelined operations

Pipe syntax allows multiple operations to be chained together. Write your operations in the same order you think about them. It's timelines all the way down, making it easy to aggregate the results of aggregations.

[source,Fenl]
----
{
  largest_spend_over_2_purchases: purchase.amount
  | when(purchase.category == "food") 
  | sum(window=sliding(2, $input)) # Inner aggregation
  | max()                          # Outer aggregation
}
----

=== Row generators

Pivot from events to time-series. Unlike grouped aggregates, generators produce rows even when there's no input, allowing you to react when something doesn't happen.

[source,Fenl]
----
{
  signups_per_hour: count(signups, window=since(daily()))
  | when(daily())
  | mean()
}
----

=== Continuous expressions

Observe the value of aggregations at arbitrary points in time. Timelines are either “discrete” (instantaneous values or events) or “continuous” (values produced by a stateful aggregations). Continuous timelines let you combine aggregates computed from different event sources.

[source,Fenl]
----
let product_average = review
| with_key(review.product_id)
| mean()

in {
  average_product_review: product_average | lookup(purchase.product_id)),
}
----

Shift values forward (but not backward) in time, allowing you to combine different temporal contexts without the risk of temporal leakage. Shifted values make it easy to compare a value “now” to a value from the past.

=== Native time travel

[source,Fenl]
----
let purchases_now = count(purchase)
let purchases_yesterday =
   purchases_now | shift_by(days(1))

in {
  purchases_in_last_day: purchases_now - purchases_yesterday,
}
----

=== Simple, composable syntax

It is functions all the way down. No global state, no dependencies to manage, and no spooky action at a distance. Quickly understand what a query is doing, and painlessly refactor to make it DRY.

[source,Fenl]
----
# How many big purchases happen each hour and where?
let cadence = hourly()
# Anything can be named and re-used
let hourly_big_purchases = purchase
| when(purchase.amount > 10)
# Filter anywhere 
| count(window=since(cadence))
# Aggregate anything
| when(cadence)
# No choosing between “when” & “having”

in {hourly_big_purchases}
# Records are just another type
| extend({
  # …modify them sequentially
  last_visit_region: last(pageview.region)
})
----

== Configuring how queries are computed

A given query can be computed in different ways.

=== Configuring how timelines are converted into tables

You can either return a table describing each change in the timeline, or a table describing the "final" value of the timeline.

Every query produces a timeline which may be returned in two different ways -- the final results (at a specific time) or all historic results.
The "result behavior" configuresj which results are produced.
Queries for historic results return the full history of how the values changed over time for each entity.
Queries for final results return the latest result for each entity at the specified time (default is after all events have been processed).

You determine which type of query to execute using the "result behavior" configuration at query time.
By default, historical results are returned.
To return final results, you must configure the `final-results` behavior:

[source,Fenl]
.Final queries with fenlmagic
----
%%fenl --result_behavior final-results
{
    time: Purchase.purchase_time,
    entity: Purchase.customer_id,
    max_amount: Purchase.amount | max(),
    min_amount: Purchase.amount | min(),
}
----

[source,python]
.Final queries with Python
----
from kaskada import compute

query = """{
  time: Purchase.purchase_time,
  entity: Purchase.customer_id,
  max_amount: last(Purchase.amount) | max(),
  min_amount: Purchase.amount | min()
}"""

resp = query.create_query(expression=query, result_behavior="final-results")
----

[source,bash]
.Final queries in the CLI
----
./kaskada-cli query run --result-behavior final-results <<EOS
Purchase
EOS
---

== Querying with Python

Using python directly is one way to write queries.

[source,python]
----
from kaskada import compute
from kaskada.api.session import LocalBuilder

session = LocalBuilder().build()

query = """{ 
  time: Purchase.purchase_time, 
  entity: Purchase.customer_id, 
  max_amount: last(Purchase.amount) | max(), 
  min_amount: Purchase.amount | min()
}"""

response_parquet = compute.query(query = query).output_to.object_store.output_paths[0]

# (Optional) view results as a Pandas dataframe.
import pandas
pandas.read_parquet(response_parquet)
----

This returns a dataframe with the results of the query.

=== Optional Parameters (with Python)

When querying directly from python, the following optional parameters
are available:

* *with_tables*: A list of tables to use in the query, in addition to
the tables stored in the system.
* *with_views*: A list of views to use in the query, in addition to the
views stored in the system.
* *result_behavior*: Determines which results are returned. Either
`"all-results"` _(default)_, or `"final-results"` which returns only the
final values for each entity.
* *response_as*: Determines how the response is returned. Either
`"parquet"` _(default)_ or `"redis-bulk"`.
** If `"redis-bulk"`, result_behavior is assumed to be
`"final-results"`.
* *data_token_id*: Enables repeatable queries. Queries performed against
the same data token always run on the same input data.
* *limits*: Configures limits on the output set.

== Querying with fenlmagic

Using the fenlmagic IPython extension makes iterating on queries easier.

[NOTE]
====
The fenlmagic IPython extension is optional and isn't required
to use Kaskada. Feel free to use whichever client interface fits your
workflow.
====

You can make Fenl queries by prefixing a query block with `%%fenl`. The
query results will be computed and returned as a Pandas dataframe. The
query content starts on the next line and includes the rest of the code
block's contents:

[source,Fenl]
----
%%fenl
{
    time: Purchase.purchase_time,
    entity: Purchase.customer_id,
    max_amount: Purchase.amount | max(),
    min_amount: Purchase.amount | min(),
}
----

This returns a dataframe with the results of the query.

=== Optional Parameters (with fenlmagic)

When querying using fenlmagic, the following optional parameters are
available:

* *--result-behavior*: Determines which results are returned. Either
`all-results` _(default)_, or `final-results` which returns only the
final values for each entity.
* *--output*: Output format for the query results. One of `df` dataframe
_(default)_, `json`, `parquet` or `redis-bulk`.
** If `redis-bulk`, --result-behavior is assumed to be `final-results`.
* *--data-token*: Enables repeatable queries. Queries performed against
the same data token always run on the same input data.
* *--preview-rows*: Produces a preview of the data with at least this
many rows.
* *--var*: Assigns the body to a local variable with the given name.

Example use of some of these options can be found in the next section:
xref:reference:example-queries[Example Queries]

=== Tables and Views

Most basic queries operate against tables. However, queries can also
operate on views or a combination of tables and views.

Here's an example of using a view to filter the values produced by an
expression using a table.

[source,Fenl]
----
%%fenl
{
  time: Purchase.purchase_time,
  entity: Purchase.customer_id,
  total_purchases: Purchase.amount | sum(),
} | when(PurchaseStats.max_amount > 100)
----

Views may reference other views, so we could give this expression a name
and create a view for it as well if we wanted to.

Views are useful any time you need to share or re-use expressions:

* Cleaning operations
* Common business logic
* Final feature vectors

For more help with tables and views, see xref:reference:tables[Working with Tables]
and xref:reference:views[Working with Views].

=== Using `--data-token`

`--data-token`: Enables repeatable queries. Queries performed against
the same data token always run on the same input data.

* use the data token id returned after loading the first file, and
results only include rows from the first file

[source,ipython]
----
%%fenl --data-token bdc9e595-a8a0-448c-9a95-c2e3d886b633
purchases
----

[source,json]
----
data_token_id: "bdc9e595-a8a0-448c-9a95-c2e3d886b633"
request_details {
  request_id: "3f737ff336666515a54dd29a9c5ace3a"
}
----

[cols=">,<,<,<,<,>,>",options="header",]
|===
| |id |purchase_time |customer_id |vendor_id |amount |subsort_id
|0 |cb_001 |2020-01-01 00:00:00 |karen |chum_bucket |9 |0
|1 |kk_001 |2020-01-01 00:00:00 |patrick |krusty_krab |3 |1
|2 |cb_002 |2020-01-02 00:00:00 |karen |chum_bucket |2 |2
|3 |kk_002 |2020-01-02 00:00:00 |patrick |krusty_krab |5 |3
|4 |cb_003 |2020-01-03 00:00:00 |karen |chum_bucket |4 |4
|5 |kk_003 |2020-01-03 00:00:00 |patrick |krusty_krab |12 |5
|6 |cb_004 |2020-01-04 00:00:00 |patrick |chum_bucket |5000 |6
|7 |cb_005 |2020-01-04 00:00:00 |karen |chum_bucket |3 |7
|8 |cb_006 |2020-01-05 00:00:00 |karen |chum_bucket |5 |8
|9 |kk_004 |2020-01-05 00:00:00 |patrick |krusty_krab |9 |9
|===

* use the data token id returned after loading the second file, and
results rows from both files

[source,ipython]
----
%%fenl --data-token 24c83cac-8cf4-4a45-98f0-dac8d5b303a2
purchases
----

[source,json]
----
data_token_id: "24c83cac-8cf4-4a45-98f0-dac8d5b303a2"
request_details {
  request_id: "3f737ff336666515a54dd29a9c5ace3a"
}
----

[cols=">,<,<,<,<,>,>",options="header",]
|===
| |id |purchase_time |customer_id |vendor_id |amount |subsort_id
|0 |cb_001 |2020-01-01 00:00:00 |karen |chum_bucket |9 |0
|1 |kk_001 |2020-01-01 00:00:00 |patrick |krusty_krab |3 |1
|2 |cb_002 |2020-01-02 00:00:00 |karen |chum_bucket |2 |2
|3 |kk_002 |2020-01-02 00:00:00 |patrick |krusty_krab |5 |3
|4 |cb_003 |2020-01-03 00:00:00 |karen |chum_bucket |4 |4
|5 |kk_003 |2020-01-03 00:00:00 |patrick |krusty_krab |12 |5
|6 |cb_004 |2020-01-04 00:00:00 |patrick |chum_bucket |5000 |6
|7 |cb_005 |2020-01-04 00:00:00 |karen |chum_bucket |3 |7
|8 |cb_006 |2020-01-05 00:00:00 |karen |chum_bucket |5 |8
|9 |kk_004 |2020-01-05 00:00:00 |patrick |krusty_krab |9 |9
|10 |kk_005 |2020-01-06 00:00:00 |patrick |krusty_krab |2 |0
|11 |wh_001 |2020-01-06 00:00:00 |spongebob |weenie_hut |7 |1
|12 |cb_007 |2020-01-07 00:00:00 |spongebob |chum_bucket |34 |2
|13 |wh_002 |2020-01-08 00:00:00 |karen |weenie_hut |8 |3
|14 |kk_006 |2020-01-08 00:00:00 |patrick |krusty_krab |9 |4
|===

=== Using `--result-behavior`

`--result-behavior`: Determines which results are returned.

* use `all-results` (default) to return all the results for each entity:

[source,ipython]
----
%%fenl --result-behavior all-results
purchases
----

[source,json]
----
data_token_id: "7bd4e740-9e63-418e-ba9b-5582db010959"
request_details {
  request_id: "1badb8b0e220e26cc15b93b234ac3c14"
}
----

[cols=">,<,<,<,<,>,>",options="header",]
|===
| |id |purchase_time |customer_id |vendor_id |amount |subsort_id
|0 |cb_001 |2020-01-01 00:00:00 |karen |chum_bucket |9 |0
|1 |kk_001 |2020-01-01 00:00:00 |patrick |krusty_krab |3 |1
|2 |cb_002 |2020-01-02 00:00:00 |karen |chum_bucket |2 |2
|3 |kk_002 |2020-01-02 00:00:00 |patrick |krusty_krab |5 |3
|4 |cb_003 |2020-01-03 00:00:00 |karen |chum_bucket |4 |4
|5 |kk_003 |2020-01-03 00:00:00 |patrick |krusty_krab |12 |5
|6 |cb_004 |2020-01-04 00:00:00 |patrick |chum_bucket |5000 |6
|7 |cb_005 |2020-01-04 00:00:00 |karen |chum_bucket |3 |7
|8 |cb_006 |2020-01-05 00:00:00 |karen |chum_bucket |5 |8
|9 |kk_004 |2020-01-05 00:00:00 |patrick |krusty_krab |9 |9
|10 |kk_005 |2020-01-06 00:00:00 |patrick |krusty_krab |2 |0
|11 |wh_001 |2020-01-06 00:00:00 |spongebob |weenie_hut |7 |1
|12 |cb_007 |2020-01-07 00:00:00 |spongebob |chum_bucket |34 |2
|13 |wh_002 |2020-01-08 00:00:00 |karen |weenie_hut |8 |3
|14 |kk_006 |2020-01-08 00:00:00 |patrick |krusty_krab |9 |4
|===

* use `final-results` to return only the most recent event for
each entity

[source,ipython]
----
%%fenl --result-behavior final-results
purchases
----

[source,json]
----
data_token_id: "7bd4e740-9e63-418e-ba9b-5582db010959"
request_details {
  request_id: "145bc51d9bac47f17fd202e5785e58b7"
}
----

[cols=">,<,<,<,<,>,>",options="header",]
|===
| |id |purchase_time |customer_id |vendor_id |amount |subsort_id
|0 |kk_006 |2020-01-08 00:00:00 |patrick |krusty_krab |9 |4
|1 |wh_002 |2020-01-08 00:00:00 |karen |weenie_hut |8 |3
|2 |cb_007 |2020-01-07 00:00:00 |spongebob |chum_bucket |34 |2
|===

=== Using `--preview-rows`

`--preview-rows`: Produces a preview of the data with approximately this many rows.

* Setting a limit allows you to quickly iterate on features and verify
your results before running them over your full dataset
* set to `50` on the `transactions` table to return a preview of at
least 50 rows

[source,ipython]
----
%%fenl --preview-rows 50
transactions
----

Returns a dataframe of 71599 rows, instead of the full dataset of 100000
rows.

[NOTE] 
====
It may seem odd that many thousands of rows were returned when
only 50 were requested. This happens because query operates on batches
and will return the results of all batches processed in order to reach
the minimum set of rows requested. In this case, compute processed only
a single batch, but the batch had a size of 71599 rows. Note: Using
`--preview-rows` with `--result-behavior final-results` will cause the
full dataset to be processed, as all inputs must be processed to produce
final results.
====