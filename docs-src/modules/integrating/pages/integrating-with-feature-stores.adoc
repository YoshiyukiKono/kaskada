= Integrating with ML feature stores

== Online Feature Stores

Models being used in production often have strict latency and freshness
requirements â€” real-time models in particular. Feature engines address
the freshness, while feature stores address latency. Kaskada's feature
engine keeps real-time features fresh and integrates with several
existing data stores for low-latency key retrieval.

=== Apache Pulsar

Apache Pulsar is an open-source distributed streaming platform.
Feature values can be written to Pulsar in real-time by creating a materialization. 

[source,python]
----
tenant = "public"
namespace = "default"
topic_name = "model_features"
broker_service_url = "pulsar://127.0.0.1:6650"
destination = materialization.PulsarDestination(tenant, namespace, topic_name, broker_service_url)

materialization.create_materialization(
    name = "MaterializedFeatures",
    destination = destination,
    query = "{
      key: Purchase.customer_id,
      max_amount: Purchase.amount | max(),
      min_amount: Purchase.amount | min(),
    }"
)
----

=== Apache Cassandra

Cassandra is a open source NoSQL distributed database that provides 
linear scalability and low-latency query results.

Feature values can be written to Cassandra in real-time via Pulsar.
First, <<apache-pulsar,materialize features into Pulsar>>.
Next use the link:https://pulsar.apache.org/docs/2.11.x/io-cassandra-sink/[Cassandra sink connector] to pull messages from Pulsar into Cassandra.

=== Redis

Redis is an OpenSource in-memory data-structure store. 

Feature values can be written to Redis in real-time via Pulsar.
First, link:Apache%20Pulsar[materialize features into Pulsar].
Next use the link:https://pulsar.apache.org/docs/2.11.x/io-redis-sink/[Redis sink connector] to pull messages from Pulsar into Redis.


== Offline Feature Stores

Models used for batch inference benefit from having access to feature
vectors in a data store optimized for throughput rather than latency.
Kaskada supports serving high-throughput feature vectors by integrating
with a number of existing data stores designed for bulk-data management.

=== Snowflake

Snowflake is a hosted data warehouse that provides scalable SQL queries
over large data sets.

Features can be loaded into Snowflake by exporting features as Parquet
and using the `COPY` instruction to load the Parquet file into a Parquet
table. To load features into Snowflake, first construct a query
describing the feature vectors. Query results can be returned as a URL
identifying the output Parquet file by supplying the output config
`--output parquet`.

[source,fenl]
----
%%fenl --output parquet
{
  key: Purchase.customer_id,
  max_amount: Purchase.amount | max(),
  min_amount: Purchase.amount | min(),
}
----

The resulting Parquet file can be loaded into a temporary Snowflake
table.

[source,sql]
----
create or replace temporary table feature_vectors (
  key varchar default null,
  max_amount number,
  min_amount number
);

create or replace file format feature_vector_parquet_format
  type = 'parquet';

create or replace temporary stage feature_vector_stage
  file_format = feature_vector_parquet_format;

put <file url> @sf_tut_stage;

copy into cities
  from (select * from @sf_tut_stage/<filename>.parquet);
----

=== Redshift

Redshift is a hosted data warehouse that provides scalable SQL queries
over large data sets.

Features can be loaded into Redshift by exporting features as Parquet
and using the `COPY` instruction to load the Parquet file into a Parquet
table. To load features into Redshift, first construct a query
describing the feature vectors. Query results can be returned as a URL
identifying the output Parquet file by supplying the output config
`--output parquet`.

[source,fenl]
----
%%fenl --output parquet
{
  key: Purchase.customer_id,
  max_amount: Purchase.amount | max(),
  min_amount: Purchase.amount | min(),
}
----

The resulting Parquet file can be loaded into a Redshift table.

[source,sql]
----
COPY feature_vectors
FROM '<file url>'
FORMAT AS PARQUET;
----
